{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: benchmarking_script (10 points)\n",
    "\n",
    "End-to-end benchmarking of forward and backward passes for `BasicsTransformerLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from a1_basics.model import BasicsTransformerLM\n",
    "from student.basicprofiling import benchmark, MODEL_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part (b): Time forward and backward passes for all model sizes\n\nUse 5 warmup steps, 10 measurement steps. Report average and standard deviation.\n\nSet `USE_SLURM_RESULTS = True` below to load pre-computed results from sbatch jobs, or `False` to run live in this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "USE_SLURM_RESULTS = True  # Toggle: True = load from sbatch CSVs, False = run live\n\nif USE_SLURM_RESULTS:\n    # --- Load pre-computed results from sbatch ---\n    import os\n    csv_path = os.path.join(os.path.dirname(\"__file__\"), \"results\", \"bench_part_b.csv\")\n    # If running on HPC, adjust path:\n    # csv_path = \"/scratch/pg2973/nyu-llm-reasoners-a2/results/bench_part_b.csv\"\n    df_b = pd.read_csv(csv_path)\n    # Normalize mode names to match notebook convention\n    df_b[\"mode\"] = df_b[\"mode\"].replace({\n        \"forward-only\": \"forward\",\n        \"forward-backward\": \"forward+backward\",\n    })\n    print(f\"Loaded {len(df_b)} rows from {csv_path}\")\n    display(df_b)\n\nelse:\n    # --- Run live in notebook ---\n    context_lengths = [128, 256, 512, 1024]\n    results = []\n\n    for size_name, params in MODEL_SIZES.items():\n        for ctx_len in context_lengths:\n            for mode in [\"forward\", \"forward+backward\"]:\n                backward = mode == \"forward+backward\"\n                print(f\"--- {size_name} | ctx={ctx_len} | {mode} ---\")\n                try:\n                    times = benchmark(\n                        d_model=params[\"d_model\"],\n                        d_ff=params[\"d_ff\"],\n                        num_layers=params[\"num_layers\"],\n                        num_heads=params[\"num_heads\"],\n                        context_length=ctx_len,\n                        warmup_steps=5,\n                        num_steps=10,\n                        backward=backward,\n                        device=DEVICE,\n                    )\n                    avg = sum(times) / len(times) * 1000\n                    std = math.sqrt(sum((t - avg / 1000) ** 2 for t in times) / len(times)) * 1000\n                    results.append({\n                        \"model\": size_name,\n                        \"ctx_len\": ctx_len,\n                        \"mode\": mode,\n                        \"avg_ms\": round(avg, 2),\n                        \"std_ms\": round(std, 2),\n                    })\n                except RuntimeError as e:\n                    print(f\"  OOM or error: {e}\")\n                    results.append({\n                        \"model\": size_name,\n                        \"ctx_len\": ctx_len,\n                        \"mode\": mode,\n                        \"avg_ms\": \"OOM\",\n                        \"std_ms\": \"OOM\",\n                    })\n                if DEVICE == \"cuda\":\n                    torch.cuda.empty_cache()\n                print()\n\n    df_b = pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_b = pd.DataFrame(results)\n\n# Pivot into a readable table: rows = (model, ctx_len), columns = mode\npivot_avg = df_b.pivot_table(index=[\"model\", \"ctx_len\"], columns=\"mode\", values=\"avg_ms\", aggfunc=\"first\")\npivot_std = df_b.pivot_table(index=[\"model\", \"ctx_len\"], columns=\"mode\", values=\"std_ms\", aggfunc=\"first\")\n\n# Combine avg ± std into a single string per cell\ndef fmt_cell(avg, std):\n    if avg == \"OOM\" or std == \"OOM\":\n        return \"OOM\"\n    return f\"{avg:.2f} ± {std:.2f}\"\n\ncombined = pd.DataFrame(index=pivot_avg.index)\nfor col in pivot_avg.columns:\n    combined[col] = [\n        fmt_cell(a, s) for a, s in zip(pivot_avg[col], pivot_std[col])\n    ]\n\ncombined.columns.name = None\ncombined = combined.rename(columns={\"forward\": \"Forward (ms)\", \"forward+backward\": \"Fwd+Bwd (ms)\"})\n\nprint(\"=== Markdown ===\")\nprint(combined.to_markdown())\nprint()\nprint(\"=== LaTeX ===\")\nprint(combined.to_latex())\n\ncombined"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) response\n",
    "\n",
    "_Fill in after running:_ A 1-2 sentence response with your timings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part (c): Effect of warm-up steps\n\nRepeat the analysis with 0, 1, 2, and 5 warm-up steps to see the effect.\n\nUses the same `USE_SLURM_RESULTS` toggle from above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_SLURM_RESULTS:\n    # --- Load pre-computed results from sbatch ---\n    csv_path = os.path.join(os.path.dirname(\"__file__\"), \"results\", \"bench_part_c.csv\")\n    # If running on HPC, adjust path:\n    # csv_path = \"/scratch/pg2973/nyu-llm-reasoners-a2/results/bench_part_c.csv\"\n    df_c = pd.read_csv(csv_path)\n    df_c[\"mode\"] = df_c[\"mode\"].replace({\n        \"forward-only\": \"forward\",\n        \"forward-backward\": \"forward+backward\",\n    })\n    print(f\"Loaded {len(df_c)} rows from {csv_path}\")\n    display(df_c)\n\nelse:\n    # --- Run live in notebook ---\n    warmup_values = [0, 1, 2, 5]\n    warmup_results = []\n\n    params = MODEL_SIZES[\"small\"]\n\n    for w in warmup_values:\n        for mode in [\"forward\", \"forward+backward\"]:\n            backward = mode == \"forward+backward\"\n            print(f\"--- warmup={w} | {mode} ---\")\n            times = benchmark(\n                d_model=params[\"d_model\"],\n                d_ff=params[\"d_ff\"],\n                num_layers=params[\"num_layers\"],\n                num_heads=params[\"num_heads\"],\n                context_length=128,\n                warmup_steps=w,\n                num_steps=10,\n                backward=backward,\n                device=DEVICE,\n            )\n            avg = sum(times) / len(times) * 1000\n            std = math.sqrt(sum((t - avg / 1000) ** 2 for t in times) / len(times)) * 1000\n            warmup_results.append({\n                \"warmup\": w,\n                \"mode\": mode,\n                \"avg_ms\": round(avg, 2),\n                \"std_ms\": round(std, 2),\n                \"first_step_ms\": round(times[0] * 1000, 2),\n            })\n            if DEVICE == \"cuda\":\n                torch.cuda.empty_cache()\n            print()\n\n    df_c = pd.DataFrame(warmup_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Normalize column name\nif \"warmup_steps\" in df_c.columns:\n    df_c = df_c.rename(columns={\"warmup_steps\": \"warmup\"})\n\npivot_avg = df_c.pivot_table(index=\"warmup\", columns=\"mode\", values=\"avg_ms\", aggfunc=\"first\")\npivot_std = df_c.pivot_table(index=\"warmup\", columns=\"mode\", values=\"std_ms\", aggfunc=\"first\")\n\ntable_c = pd.DataFrame(index=pivot_avg.index)\nfor col in pivot_avg.columns:\n    table_c[f\"{col} avg±std (ms)\"] = [\n        f\"{a:.2f} ± {s:.2f}\" for a, s in zip(pivot_avg[col], pivot_std[col])\n    ]\n\n# Add first-step column if available (live runs only)\nif \"first_step_ms\" in df_c.columns:\n    pivot_first = df_c.pivot_table(index=\"warmup\", columns=\"mode\", values=\"first_step_ms\", aggfunc=\"first\")\n    for col in pivot_first.columns:\n        table_c[f\"{col} 1st step (ms)\"] = [f\"{v:.2f}\" for v in pivot_first[col]]\n\ntable_c.columns.name = None\ntable_c.index.name = \"warmup\"\n\nprint(\"=== Markdown ===\")\nprint(table_c.to_markdown())\nprint()\nprint(\"=== LaTeX ===\")\nprint(table_c.to_latex())\n\ntable_c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) response\n",
    "\n",
    "_Fill in after running:_ A 2-3 sentence response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}